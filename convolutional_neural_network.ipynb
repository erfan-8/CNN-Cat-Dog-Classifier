{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "convolutional_neural_network.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DR-eO17geWu",
    "colab_type": "text"
   },
   "source": [
    "# Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMefrVPCg-60",
    "colab_type": "text"
   },
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:28.119573Z",
     "start_time": "2025-10-27T00:19:28.105758Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:28.156017Z",
     "start_time": "2025-10-27T00:19:28.135395Z"
    }
   },
   "cell_type": "code",
   "source": "tf.__version__",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.20.0'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxQxCBWyoGPE",
    "colab_type": "text"
   },
   "source": [
    "## Part 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvE-heJNo3GG",
    "colab_type": "text"
   },
   "source": [
    "### Preprocessing the Training set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:33.736992Z",
     "start_time": "2025-10-27T00:19:32.130722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "training_set = train_datagen.flow_from_directory('dataset/training_set',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'binary')"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 21185 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrCMmGw9pHys",
    "colab_type": "text"
   },
   "source": [
    "### Preprocessing the Test set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:53:05.584048Z",
     "start_time": "2025-10-27T00:53:05.347861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "test_set = test_datagen.flow_from_directory('dataset/test_set',\n",
    "                                            target_size = (64, 64),\n",
    "                                            batch_size = 32,\n",
    "                                            class_mode = 'binary',\n",
    "                                            shuffle = False)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3799 images belonging to 2 classes.\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af8O4l90gk7B",
    "colab_type": "text"
   },
   "source": [
    "## Part 2 - Building the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ces1gXY2lmoX",
    "colab_type": "text"
   },
   "source": [
    "### Initialising the CNN"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:38.024114Z",
     "start_time": "2025-10-27T00:19:38.003843Z"
    }
   },
   "cell_type": "code",
   "source": "cnn = tf.keras.models.Sequential()",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5YJj_XMl5LF",
    "colab_type": "text"
   },
   "source": [
    "### Step 1 - Convolution"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:39.976706Z",
     "start_time": "2025-10-27T00:19:39.897079Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu',input_shape=(64, 64, 3)))",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIRA\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf87FpvxmNOJ",
    "colab_type": "text"
   },
   "source": [
    "### Step 2 - Pooling"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:47.940932Z",
     "start_time": "2025-10-27T00:19:47.916514Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))",
   "outputs": [],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xaTOgD8rm4mU",
    "colab_type": "text"
   },
   "source": [
    "### Adding a second convolutional layer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:49.392751Z",
     "start_time": "2025-10-27T00:19:49.361115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "cnn.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), strides=2))"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tmiEuvTunKfk",
    "colab_type": "text"
   },
   "source": [
    "### Step 3 - Flattening"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:50.476536Z",
     "start_time": "2025-10-27T00:19:50.457993Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Flatten())",
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAoSECOm203v",
    "colab_type": "text"
   },
   "source": [
    "### Step 4 - Full Connection"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:52.384550Z",
     "start_time": "2025-10-27T00:19:52.360734Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))",
   "outputs": [],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTldFvbX28Na",
    "colab_type": "text"
   },
   "source": [
    "### Step 5 - Output Layer"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:53.393233Z",
     "start_time": "2025-10-27T00:19:53.374019Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))",
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6XkI90snSDl",
    "colab_type": "text"
   },
   "source": [
    "## Part 3 - Training the CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfrFQACEnc6i",
    "colab_type": "text"
   },
   "source": [
    "### Compiling the CNN"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:19:54.858323Z",
     "start_time": "2025-10-27T00:19:54.816192Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])",
   "outputs": [],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehS-v3MIpX2h",
    "colab_type": "text"
   },
   "source": [
    "### Training the CNN on the Training set and evaluating it on the Test set"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:50:24.953708Z",
     "start_time": "2025-10-27T00:19:56.652758Z"
    }
   },
   "cell_type": "code",
   "source": "cnn.fit(x=training_set, validation_data=test_set, epochs=25)",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIRA\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 110ms/step - accuracy: 0.5886 - loss: 0.6677"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VIRA\\anaconda3\\envs\\tf-gpu\\lib\\site-packages\\PIL\\TiffImagePlugin.py:858: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m87s\u001B[0m 130ms/step - accuracy: 0.6444 - loss: 0.6271 - val_accuracy: 0.7378 - val_loss: 0.5190\n",
      "Epoch 2/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m78s\u001B[0m 117ms/step - accuracy: 0.7409 - loss: 0.5246 - val_accuracy: 0.7689 - val_loss: 0.4884\n",
      "Epoch 3/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 109ms/step - accuracy: 0.7682 - loss: 0.4835 - val_accuracy: 0.8134 - val_loss: 0.4201\n",
      "Epoch 4/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m65s\u001B[0m 98ms/step - accuracy: 0.7896 - loss: 0.4536 - val_accuracy: 0.7992 - val_loss: 0.4350\n",
      "Epoch 5/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m67s\u001B[0m 100ms/step - accuracy: 0.7987 - loss: 0.4341 - val_accuracy: 0.7715 - val_loss: 0.4763\n",
      "Epoch 6/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m68s\u001B[0m 103ms/step - accuracy: 0.8040 - loss: 0.4230 - val_accuracy: 0.8105 - val_loss: 0.4219\n",
      "Epoch 7/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m81s\u001B[0m 122ms/step - accuracy: 0.8107 - loss: 0.4090 - val_accuracy: 0.7984 - val_loss: 0.4300\n",
      "Epoch 8/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8234 - loss: 0.3886 - val_accuracy: 0.8321 - val_loss: 0.3837\n",
      "Epoch 9/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 107ms/step - accuracy: 0.8315 - loss: 0.3756 - val_accuracy: 0.8068 - val_loss: 0.4147\n",
      "Epoch 10/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8362 - loss: 0.3601 - val_accuracy: 0.8207 - val_loss: 0.3919\n",
      "Epoch 11/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8381 - loss: 0.3517 - val_accuracy: 0.8492 - val_loss: 0.3494\n",
      "Epoch 12/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m84s\u001B[0m 126ms/step - accuracy: 0.8498 - loss: 0.3359 - val_accuracy: 0.8329 - val_loss: 0.3843\n",
      "Epoch 13/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8538 - loss: 0.3279 - val_accuracy: 0.8210 - val_loss: 0.3908\n",
      "Epoch 14/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8600 - loss: 0.3178 - val_accuracy: 0.8460 - val_loss: 0.3492\n",
      "Epoch 15/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 107ms/step - accuracy: 0.8633 - loss: 0.3083 - val_accuracy: 0.8494 - val_loss: 0.3579\n",
      "Epoch 16/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m84s\u001B[0m 127ms/step - accuracy: 0.8726 - loss: 0.2955 - val_accuracy: 0.8500 - val_loss: 0.3537\n",
      "Epoch 17/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8785 - loss: 0.2841 - val_accuracy: 0.8450 - val_loss: 0.3679\n",
      "Epoch 18/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 108ms/step - accuracy: 0.8826 - loss: 0.2809 - val_accuracy: 0.8529 - val_loss: 0.3633\n",
      "Epoch 19/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m72s\u001B[0m 108ms/step - accuracy: 0.8882 - loss: 0.2612 - val_accuracy: 0.8468 - val_loss: 0.3597\n",
      "Epoch 20/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 107ms/step - accuracy: 0.8901 - loss: 0.2562 - val_accuracy: 0.8413 - val_loss: 0.3898\n",
      "Epoch 21/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m82s\u001B[0m 124ms/step - accuracy: 0.8927 - loss: 0.2538 - val_accuracy: 0.8271 - val_loss: 0.4192\n",
      "Epoch 22/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 106ms/step - accuracy: 0.8989 - loss: 0.2432 - val_accuracy: 0.8550 - val_loss: 0.3612\n",
      "Epoch 23/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 108ms/step - accuracy: 0.8999 - loss: 0.2374 - val_accuracy: 0.8605 - val_loss: 0.3509\n",
      "Epoch 24/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m70s\u001B[0m 105ms/step - accuracy: 0.9045 - loss: 0.2310 - val_accuracy: 0.8465 - val_loss: 0.3922\n",
      "Epoch 25/25\n",
      "\u001B[1m663/663\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m71s\u001B[0m 106ms/step - accuracy: 0.9052 - loss: 0.2242 - val_accuracy: 0.8505 - val_loss: 0.3767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x19d7eb33520>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3PZasO0006Z",
    "colab_type": "text"
   },
   "source": [
    "## Part 4 - Making a single prediction"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:54:10.941623Z",
     "start_time": "2025-10-27T00:54:10.841604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "test_image = image.load_img('dataset/single_prediction/4.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = cnn.predict(test_image)\n",
    "training_set.class_indices\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'dog'\n",
    "else:\n",
    "    prediction = 'cat'"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m1/1\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 34ms/step\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:54:12.819809Z",
     "start_time": "2025-10-27T00:54:12.811841Z"
    }
   },
   "cell_type": "code",
   "source": "print(prediction)",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-27T00:55:43.444432Z",
     "start_time": "2025-10-27T00:55:36.353452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "\n",
    "print(\"\\nStarting detailed evaluation on the test set...\")\n",
    "y_true = test_set.classes\n",
    "test_set.reset()\n",
    "\n",
    "steps_to_predict = int(np.ceil(test_set.samples / test_set.batch_size))\n",
    "\n",
    "y_pred_probs = cnn.predict(test_set, steps=steps_to_predict)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "class_indices = training_set.class_indices\n",
    "class_names = list(class_indices.keys())\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(cm)\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=class_names))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting detailed evaluation on the test set...\n",
      "\u001B[1m119/119\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m7s\u001B[0m 56ms/step\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1140  260]\n",
      " [ 308 2091]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Cat       0.79      0.81      0.80      1400\n",
      "         Dog       0.89      0.87      0.88      2399\n",
      "\n",
      "    accuracy                           0.85      3799\n",
      "   macro avg       0.84      0.84      0.84      3799\n",
      "weighted avg       0.85      0.85      0.85      3799\n",
      "\n"
     ]
    }
   ],
   "execution_count": 26
  }
 ]
}
